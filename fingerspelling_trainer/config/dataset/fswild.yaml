defaults:
  - transformations: transformations_ASL
  - alphabet: english

data_path: /home/gts/projects/lruanova/projects/signamed/data/ASL/transformed
num_workers_dataloader: 4

# --- sampler fswild
sampler:
  enabled:       true
  mode: freq # original | freq
  dataset_multiplier: 1
  filename:      "fswild_weights_sampler.pt"
  base_boost:    1.0
  motion_tokens: ["Z",]
  motion_boost: 5.0
  rare_tokens: ["Q", "X"]
  rare_boost: 3.0
  use_fpc:       false
  fpc_boosts:    [1,1,1,1,1] # ignored
  clip_value:    15.0
  alphabet: ${dataset.alphabet}


training:
  default_root_dir: /home/gts/projects/lruanova/refactor_fingerspelling/
  run_test_after_train: true

  seed: 42
  batch_size: 8
  max_epochs: 10
  gradient_max_norm : 1.0

  optimizer:
    obj:
      _target_: adamp.AdamP
    learning_rate: 5e-4
    weight_decay : 5e-4
    cosine_decay: True
    warmup_epochs: 2
    schedule_predictors: False # if to apply lr schedule to predictors

  callbacks:
    ray_train_report:
      active: true
      obj:
        _target_: ray.train.lightning.RayTrainReportCallback
    early_stopping:
      active: true
      obj:
        _target_: lightning.pytorch.callbacks.EarlyStopping
        monitor: "val/loss"

        patience: 4

    model_checkpoint:
      active: true
      obj:
        _target_: lightning.pytorch.callbacks.ModelCheckpoint
        monitor: "val/loss"
        save_top_k: 1
        mode: "min"
        filename: "{epoch:02d}-{val/loss:.4f}"

    ctc_ramp:
      active: ${learner.ctc.type} != "standard"
      obj:
        _target_: fingerspelling_trainer.training.utils.ctc_ramp_callback.CTCRampCallback
        ctc_type: ${learner.ctc.type}
        start_epoch: ${learner.ctc.warmup_epochs}
        ramp_epochs: ${learner.ctc.ramp_epochs}
        gamma_final: ${learner.ctc.gamma}
        verbose_name: ${learner.ctc.type}

    swa:
      active: true
      obj:
        _target_: lightning.pytorch.callbacks.StochasticWeightAveraging
        swa_lrs: 1e-4  # learning rate when active
        swa_epoch_start: 0.5  # % of training

  scheduler: # TODO: Hardcoded
    eta_min: 0.0
